{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A cookbook for Named Entities Recognition with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminary steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. import information from the config file\n",
    "\n",
    "Here you store 3 variables with:\n",
    "\n",
    "* the name of the file\n",
    "* the column with ids\n",
    "* the column with texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "my_file = cfg['museum']['file_name']\n",
    "id_ = cfg['museum']['id_column_name']\n",
    "txt = cfg['museum']['text_column_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. open the file and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(my_file)\n",
    "records = [{'_id':row[id_],'text': row[txt]} for _, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Try the demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. implement the model\n",
    "\n",
    "In this section we implement the model. You can play around with different models by changing their huggingface path in  the config.yaml file\n",
    "\n",
    "\n",
    "global: <br>\n",
    "    model_name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch,accelerate,regex as re\n",
    "\n",
    "model_name = cfg['global']['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create the prompt\n",
    "Here we build the prompt. Give a look to the example in the config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_item = cfg['inference']['text']\n",
    "prompt = cfg['inference']['prompt'].format(text=text_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3a. Do the inference [NO MAC]\n",
    "Here we do the inference if we do not have a Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                output_scores=True,\n",
    "                temperature=0.3,\n",
    "            )\n",
    "\n",
    "        decoded_text = tokenizer.decode(\n",
    "                outputs[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        model_output = decoded_text.split(\"Answer:\")[-1]\n",
    "        print(model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3b. Do the inference [ONLY MAC]\n",
    "\n",
    "Here we do the inference if we have a mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```json\n",
      "{'PERSON': ['Abarth', 'Riccardo Patrese', 'Michele Alboreto', 'Nicola Larini'], 'LOCATION': ['CSAI', 'Fiat 124 Sport', 'Formula Italia Trophy']}\n",
      "```\n",
      "```json\n",
      "{'PERSON': ['Abarth', 'Riccardo Patrese', 'Michele Alboreto', 'Nicola Larini'], 'LOCATION': ['CSAI', 'Fiat 124 Sport', 'Formula Italia Trophy']}\n",
      "```\n",
      "```json\n",
      "{'PERSON': ['Abarth', 'Riccardo Patrese', 'Michele Alboreto', 'Nicola Larini'], 'LOCATION': ['CS\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('mps')\n",
    "\n",
    "with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                output_scores=True,\n",
    "                temperature=0.3,\n",
    "            ).to('mps')\n",
    "\n",
    "        decoded_text = tokenizer.decode(\n",
    "                outputs[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        model_output = decoded_text.split(\"Answer:\")[-1]\n",
    "        print(model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assignment\n",
    "\n",
    "1. play with different models and parameters in the config file\n",
    "2. implement the script to extract entities from all the descriptions (you can also try to change entities, if you want)\n",
    "3. save everything in a csv with three columns: _id, entity, entity_type (eg. abarth-c-se-025-formula-italia, Riccardo Patrese, PERSON) and save it in the output folder\n",
    "4. materialize a KG with SPARQL anything and save it in the output folder\n",
    "5. create a new branch in the Github repository with your name or the name of your team and send a mail to professor Damiano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4ch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
